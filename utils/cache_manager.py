# utils/cache_manager.py
import os
import joblib
from threading import Lock
from concurrent.futures import ThreadPoolExecutor

from utils.config import OUTPUT_DIR, DATASET_CONFIGS

class CacheManager:
    _instance = None
    _lock = Lock()

    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if not cls._instance:
                cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, 'cache'):
            self.cache = {}
            print("CacheManager initialized.")

    # ==================== ÙŠØ¨Ø¯Ø£ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§ ====================

    def _load_single_model(self, task: tuple) -> tuple:
        """
        Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯. Ù…ØµÙ…Ù…Ø© Ù„ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²Ù.
        ØªØ±Ø¬Ø¹ Ù…ÙØªØ§Ø­ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù…Ø­Ù…Ù„Ø©.
        """
        dataset, model_type = task
        try:
            model_dir = os.path.join(OUTPUT_DIR, dataset, model_type)
            if not os.path.exists(model_dir):
                print(f"  âš ï¸ Directory not found for {model_type}/{dataset}, skipping.")
                return dataset, model_type, None

            print(f"  - Starting load for {model_type}/{dataset}...")
            assets = {
                "vectorizer": joblib.load(os.path.join(model_dir, "vectorizer.joblib")),
                "matrix": joblib.load(os.path.join(model_dir, "matrix.joblib")),
                "doc_ids_map": joblib.load(os.path.join(model_dir, "doc_ids_map.joblib"))
            }
            print(f"  âœ… Finished load for {model_type}/{dataset}.")
            return dataset, model_type, assets
        except Exception as e:
            print(f"  âŒ Error loading assets for {model_type}/{dataset}: {e}")
            return dataset, model_type, None

    async def load_all_models(self):
        """
        Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©: ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ ÙƒÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ø²ÙŠ.
        """
        if self.cache:
            print("Models are already loaded. Skipping.")
            return

        print("ğŸš€ Central CacheManager: Starting PARALLEL pre-load of all models...")
        
        # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¦Ù…Ø© Ø¨ÙƒÙ„ Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ ØªÙ†ÙÙŠØ°Ù‡Ø§
        tasks = []
        for dataset in DATASET_CONFIGS.keys():
            self.cache[dataset] = {} # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù‚Ø§Ù…ÙˆØ³
            for model_type in ["tfidf", "bert"]:
                tasks.append((dataset, model_type))

        # 2. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…Ø¹ Ø®ÙŠÙˆØ· (Thread Pool) ÙˆØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ø§Ù… Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        # Ø³ÙŠÙ‚ÙˆÙ… Ø¨ØªØ´ØºÙŠÙ„ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ù…Ù‡Ø§Ù… ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª (Ø­ØªÙ‰ 10 Ù‡Ù†Ø§)
        with ThreadPoolExecutor(max_workers=10) as executor:
            # executor.map ÙŠØ·Ø¨Ù‚ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ù‡Ù…Ø© ÙˆÙŠØ¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            results = executor.map(self._load_single_model, tasks)

        # 3. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆÙˆØ¶Ø¹Ù‡Ø§ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø®Ø¨Ø£Ø©
        for dataset, model_type, loaded_assets in results:
            if loaded_assets:
                self.cache[dataset][model_type] = loaded_assets
        
        print("âœ… Central CacheManager: All models loaded in parallel.")

    # ==================== ÙŠÙ†ØªÙ‡ÙŠ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§ ====================
    
    def get_assets(self, dataset: str, model_type: str):
        """
        Retrieves the assets for a specific model and dataset from the cache.
        """
        return self.cache.get(dataset, {}).get(model_type)